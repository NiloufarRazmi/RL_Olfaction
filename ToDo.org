#+title: To-Do
* DONE Remote GPU
** DONE Run as batch script
*** DONE Run Jupytext converted script
*** DONE Convert training as a script with arguments
**** Notebook should be refactored for loading weights, inference and plotting
**** Example:
[[https://github.com/NICALab/SUPPORT/blob/main/src/train.py][~train.py~ example script]] which can be run with ~python -m src.train --exp_name mytest --n_epochs 11 --checkpoint_interval 10~
** Setup Tensorboard
*** Tuto
- https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html
- https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html
*** KILL + Make tutorial for Oscar?
** DONE Save everything to disk
*** DONE Save all the data in file
*** DONE Plot from file
*** DONE Save plot in files
*** DONE Save progressbar log in file
*** Use an MLOps tool like MLFlow or AimStack to save everything?
** Implement resume training feature
* TODO Viz
** DONE Improve loss plot?
Make number of bins not relative to the numer of episodes?

** KILL Get info during training
*** KILL Either screen + interactive session
*** Or batch script that writes in a file
Using  Tensorboard ~SummaryWriter~
** DONE Plot neurons activations
Get inspiration from https://pair.withgoogle.com/explorables/grokking/
** Plot weights matrices
See Figure 1 from [[https://arxiv.org/pdf/2007.02686.pdf]]
*** Put colorbar for 2 plots (weights + biases) at the bottom?
[[https://jdhao.github.io/2017/06/11/mpl_multiplot_one_colorbar/]]
*** Get colorbar values for 2 plots
[[https://stackoverflow.com/a/52837393/4129062]]
** DONE Plot policy learned
** Add loss info in progress bar
[[https://aladdinpersson.medium.com/how-to-get-a-progress-bar-in-pytorch-72bdbf19b35c]]
** Plot loss updates during training
** Plot distributions
#+begin_src python
##############################################################


def plot_dists(val_dict, color="C0", xlabel=None, stat="count", use_kde=True):
    columns = len(val_dict)
    fig, ax = plt.subplots(1, columns, figsize=(columns * 3, 2.5))
    fig_index = 0
    for key in sorted(val_dict.keys()):
        key_ax = ax[fig_index % columns]
        sns.histplot(
            val_dict[key],
            ax=key_ax,
            color=color,
            bins=50,
            stat=stat,
            kde=use_kde and ((val_dict[key].max() - val_dict[key].min()) > 1e-8),
        )  # Only plot kde if there is variance
        hidden_dim_str = (
            r"(%i $\to$ %i)" % (val_dict[key].shape[1], val_dict[key].shape[0]) if len(val_dict[key].shape) > 1 else ""
        )
        key_ax.set_title(f"{key} {hidden_dim_str}")
        if xlabel is not None:
            key_ax.set_xlabel(xlabel)
        fig_index += 1
    fig.subplots_adjust(wspace=0.4)
    return fig


##############################################################


def visualize_weight_distribution(model, color="C0"):
    weights = {}
    for name, param in model.named_parameters():
        if name.endswith(".bias"):
            continue
        key_name = f"Layer {name.split('.')[1]}"
        weights[key_name] = param.detach().view(-1).cpu().numpy()

    # Plotting
    fig = plot_dists(weights, color=color, xlabel="Weight vals")
    fig.suptitle("Weight distribution", fontsize=14, y=1.05)
    plt.show()
    plt.close()


##############################################################


def visualize_gradients(model, color="C0", print_variance=False):
    """
    Args:
        net: Object of class BaseNetwork
        color: Color in which we want to visualize the histogram (for easier separation of activation functions)
    """
    model.eval()
    small_loader = data.DataLoader(train_set, batch_size=1024, shuffle=False)
    imgs, labels = next(iter(small_loader))
    imgs, labels = imgs.to(device), labels.to(device)

    # Pass one batch through the network, and calculate the gradients for the weights
    model.zero_grad()
    preds = model(imgs)
    loss = F.cross_entropy(preds, labels)  # Same as nn.CrossEntropyLoss, but as a function instead of module
    loss.backward()
    # We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots
    grads = {
        name: params.grad.view(-1).cpu().clone().numpy()
        for name, params in model.named_parameters()
        if "weight" in name
    }
    model.zero_grad()

    # Plotting
    fig = plot_dists(grads, color=color, xlabel="Grad magnitude")
    fig.suptitle("Gradient distribution", fontsize=14, y=1.05)
    plt.show()
    plt.close()

    if print_variance:
        for key in sorted(grads.keys()):
            print(f"{key} - Variance: {np.var(grads[key])}")


##############################################################


def visualize_activations(model, color="C0", print_variance=False):
    model.eval()
    small_loader = data.DataLoader(train_set, batch_size=1024, shuffle=False)
    imgs, labels = next(iter(small_loader))
    imgs, labels = imgs.to(device), labels.to(device)

    # Pass one batch through the network, and calculate the gradients for the weights
    feats = imgs.view(imgs.shape[0], -1)
    activations = {}
    with torch.no_grad():
        for layer_index, layer in enumerate(model.layers):
            feats = layer(feats)
            if isinstance(layer, nn.Linear):
                activations[f"Layer {layer_index}"] = feats.view(-1).detach().cpu().numpy()

    # Plotting
    fig = plot_dists(activations, color=color, stat="density", xlabel="Activation vals")
    fig.suptitle("Activation distribution", fontsize=14, y=1.05)
    plt.show()
    plt.close()

    if print_variance:
        for key in sorted(activations.keys()):
            print(f"{key} - Variance: {np.var(activations[key])}")


##############################################################
#+end_src
[[https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/03-initialization-and-optimization.html]]
** 2D Weights histogram?
[[https://wandb.ai/sauravmaheshkar/initialization/reports/A-Gentle-Introduction-To-Weight-Initialization-for-Neural-Networks--Vmlldzo2ODExMTg#he-initialization-]]
** Plot policy animation
** DONE Divide policy plot in upper/lower triangle
** Plot clustermap before training?
** PCA of weights/activations?
*** 10.48550/arXiv.2402.12067
** TODO Log of the Q-value to get better color? Only for the colormap?
** TODO Plot 3 activations matrices (by condition)?
* XAI/Analysis ideas
** Frequency/FFT?
10.48550/arXiv.2403.02241
** https://www.astralcodexten.com/p/god-help-us-lets-try-to-understand
** SHAP values/LIME?
* TODO Experiments
** Representations
*** Look at the last layer
*** Split Hyp and olfactory cortex in 2 layers
**** LEC would be output
**** How-to:
- https://discuss.pytorch.org/t/best-way-to-split-process-merge/18702
- https://pytorch.org/docs/stable/generated/torch.tensor_split.html
- https://discuss.pytorch.org/t/combine-linear-layers/22337/3
** Test input fixed replay buffer with all the right transitions
** DONE Check 50% chance if 100% random actions + add CI test
** DONE Test generalization if agent trained only in upper triangle and later trained in lower triangle
Should learn faster than agent trained only in the lower triangle
** DONE Light cue in the state?
** DONE No ReLU at the last layer?
** Normalizing states/actions?
** Reward at 1 instead of 10?
** DONE Start training when replay buffer is full?
** Minimum representations topic
*** Compress to a 3? neurons layer and look at the compressed representations learned
*** Find the smallest network able to learn the task (single hidden layer, X neurons), and look at the representations learned
*** Check the neurons activity and compare to the normal/fully connected case
** KILL Memory in the environment + RNN?
*** Only pass the odor information at the port, not at each time step
** TODO Polar/Cartesian experiment
*** DONE Code
**** DONE Merge action space for ego/allo
Allow moving backward?
**** Make functions to compute polar and Cartesian coordinates from different landmarks
**** Create function to switch between left/right & east/west task?
**** DONE Remove rounding on input values, pass true floating value to the network?
***** DQN with continuous state space?
Yes: neural network with enough capacity should appproximate the continuous function
***** DQN with continuous action space?
No: ~argmax~ in the cost function, so better to have it discretized
**** DONE Angle relative from the port
- Difference from the agent to the port
- 2x head direction angles for Cartesian coordinates + 2x head direction angles for polar coordinates +
**** DONE head direction relative to the agent
- Need to keep a fixed internal direction?
- But feed a relative head direction to the network?
**** DONE Remove ~backward~ action
*** DONE Baseline
**** DONE 2 Cartesian + 2 polar with symmetric orientation
**** DONE Coordinate systems at North and South for now
**** Then discretized version
*** Exp. 1: How the constraints of the task impact learning?
**** Train left/right task
**** Train east/west task
**** Analyze the activity learned on polar vs. Cartesian representations
*** Exp. 2: Does the network learn a coordinate system?
**** Train on the normal task, then move the agent to a translated coordinate system, i.e. same values but more interpolated points between 2 discreet values
- Same experiment also on translated polar coordinates?
- Both left/right and east/west task?
- Only input Cartesian coordinates into the network?
*** KILL Exp. 3: does having redundant info makes the agent more robust in a noisy environment?
**** Train on the normal task, then after training feed incorrect polar angles, i.e. rotated by X degree
- Same experiment also on incorrect Cartesian coordinates?
- Both left/right and east/west task?
*** TODO Analysis
**** DONE Plot activation in the last layer
***** DONE Last layer before or after activation, i.e. layer 6 or 7?
DQN(
  (mlp): Sequential(
    (0): Linear(in_features=10, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=512, bias=True)
    (5): ReLU()
    (6): Linear(in_features=512, out_features=512, bias=True)
    (7): ReLU()
    (8): Linear(in_features=512, out_features=3, bias=True)
  )
)
\to Layer 6 as there are no parameters in ReLU
**** TODO Plot the activations on tile/coordinates map
**** TODO Find Cartesian/polar neurons
**** TODO Sort neurons by Cartesian/polar
* DONE Debug
** DONE Unit test env
** DONE Unit test one hot case
** DONE Test learning gradually
*** only get to reward with no odor
*** always odor A
*** full
** DONE Not max but Q of the chosen action
** DONE Check update rule
** DONE Vector or zeros instead of scalar Q value in the loss function
** DONE One hot encoding of state inputs
** DONE Plot stats of weights and biases
** DONE Plot gradients
* Improvements
** DONE Batches
** DONE Experience replay
** DONE Target network
** DONE Replace list type for replay buffer by ~dequeu()~ or ~NamedTuple~?
** DONE Add \epsilon-greedy starting from ~\epsilon=1~ + add test
** KILL Modify warm up episodes to warm up steps
** DONE Soft update of the target network's weights
** DONE Huber loss
** [?] \epsilon-greedy
*** Linear decrease with slope aiming at number of episodes max
*** Adaptive \epsilon-greedy
[[https://doi.org/10.1016/j.procs.2017.05.431]]
** DONE Seed everything to be able to test runs that don't work
*** [[https://docs.python.org/3/using/cmdline.html#envvar-PYTHONHASHSEED][PYTHONHASHSEED]]
** Unit test DQN algo
[[https://krokotsch.eu/posts/deep-learning-unit-tests/]]
*** Test that the loss decreases
*** Test that the weights are updated
*** Test the shapes of the data, input/output to the network, and of all tensors
*** Check gradients are not zero after one step of backprop
*** Check replay buffer is being sampled from correctly
*** Make target network update frequency infinite to see whether Q-network converges
** DONE Add light cue to the state?
** Running is slow
*** DONE Implement checkpointing to save the model parameters and not have to start from scratch everytime
[[https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training]]
*** Profile code on the GPU
*** DONE Simplify model
**** DONE Remove target network?
**** DONE Remove replay traces?
**** DONE Replace decaying \epsilon-greedy with my own
** Incrementally save after each run?
** DONE Save ~params.ini~ file
** DONE Add task as boolean to be chosen in conf file
** DONE Add task name in directory name
** DONE Revert commit about experiment tag in conf name
* Metrics
** DONE Reward histogram
** average entropy of action distribution
** DONE Add logging
- [[https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html]]
- [[https://dvc.org/doc/dvclive/ml-frameworks/pytorch]]
* TODO Questions
** TODO Why bump in the number of steps?
Bin distribution to understand if agent is exploiting/exploring
