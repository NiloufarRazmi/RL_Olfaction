#+title: To-Do
* Running is slow
** DONE Implement checkpointing to save the model parameters and not have to start from scratch everytime
[[https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training]]
** Profile code on the GPU
** DONE Simplify model
*** DONE Remove target network?
*** DONE Remove replay traces?
*** DONE Replace decaying \epsilon-greedy with my own
* Remote GPU
** Run as batch script
*** DONE Run Jupytext converted script
*** Convert training as a script with arguments
**** Notebook should be refactored for loading weights, inference and plotting
**** Example:
[[https://github.com/NICALab/SUPPORT/blob/main/src/train.py][~train.py~ example script]] which can be run with ~python -m src.train --exp_name mytest --n_epochs 11 --checkpoint_interval 10~
** Setup Tensorboard
*** Tuto
- https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html
- https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html
*** KILL + Make tutorial for Oscar?
* Viz
** DONE Improve loss plot?
Make number of bins not relative to the numer of episodes?

** Get info during training training
*** TODO Either screen + interactive session
*** WAIT Or batch script that writes in a file
Using  Tensorboard ~SummaryWriter~
** Plot weights and neurons activations?
Get inspiration from https://pair.withgoogle.com/explorables/grokking/
** WAIT Plot policy learned
* DONE Model doesn't converge
** DONE Try to apply on simple MDP
* Experiments
** Representations
*** Look at the last layer
*** Split Hyp and olfactory cortex in 2 layers
**** LEC would be output
**** How-to:
- https://discuss.pytorch.org/t/best-way-to-split-process-merge/18702
- https://pytorch.org/docs/stable/generated/torch.tensor_split.html
- https://discuss.pytorch.org/t/combine-linear-layers/22337/3
