#+title: To-Do
* Running is slow
** DONE Implement checkpointing to save the model parameters and not have to start from scratch everytime
[[https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training]]
** Profile code on the GPU
** DONE Simplify model
*** DONE Remove target network?
*** DONE Remove replay traces?
*** DONE Replace decaying \epsilon-greedy with my own
* Remote GPU
** Run as batch script
*** DONE Run Jupytext converted script
*** Convert training as a script with arguments
**** Notebook should be refactored for loading weights, inference and plotting
**** Example:
[[https://github.com/NICALab/SUPPORT/blob/main/src/train.py][~train.py~ example script]] which can be run with ~python -m src.train --exp_name mytest --n_epochs 11 --checkpoint_interval 10~
** Setup Tensorboard
*** Tuto
- https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html
- https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html
*** KILL + Make tutorial for Oscar?
* TODO Viz
** DONE Improve loss plot?
Make number of bins not relative to the numer of episodes?

** Get info during training training
*** TODO Either screen + interactive session
*** WAIT Or batch script that writes in a file
Using  Tensorboard ~SummaryWriter~
** TODO Plot weights and neurons activations
Get inspiration from https://pair.withgoogle.com/explorables/grokking/
** DONE Plot policy learned
* DONE Model doesn't converge
** DONE Try to apply on simple MDP
* TODO Experiments
** Representations
*** Look at the last layer
*** Split Hyp and olfactory cortex in 2 layers
**** LEC would be output
**** How-to:
- https://discuss.pytorch.org/t/best-way-to-split-process-merge/18702
- https://pytorch.org/docs/stable/generated/torch.tensor_split.html
- https://discuss.pytorch.org/t/combine-linear-layers/22337/3
** TODO Test input fixed replay buffer with all the transitions
** TODO Check 50% chance if 100% random actions + add CI test
* DONE Debug
** DONE Unit test env
** DONE Unit test one hot case
** DONE Test learning gradually
*** only get to reward with no odor
*** always odor A
*** full
** DONE Not max but Q of the chosen action
** DONE Check update rule
** DONE Vector or zeros instead of scalar Q value in the loss function
** DONE One hot encoding of state inputs
** DONE Plot stats of weights and biases
** DONE Plot gradients
* TODO Improvements
** TODO Minibatches
** DONE Experience replay
** Target network
** Replace list type for replay buffer by ~dequeu()~
** DONE Add \epsilon-greedy starting from ~\epsilon=1~ + add test
** Metrics
*** Reward histogram
*** average entropy of action distribution
