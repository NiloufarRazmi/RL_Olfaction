{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54AIvDov_7aa"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gxxpHDIs_lvg",
    "outputId": "412c65c1-7c24-4b9a-f5cf-8f98dd648fe4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import ipdb\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imojify import imojify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gxxpHDIs_lvg",
    "outputId": "412c65c1-7c24-4b9a-f5cf-8f98dd648fe4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import Params, random_choice\n",
    "from environment_tensor import WrappedEnvironment, Actions, CONTEXTS_LABELS, Cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gxxpHDIs_lvg",
    "outputId": "412c65c1-7c24-4b9a-f5cf-8f98dd648fe4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Formatting & autoreload stuff\n",
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gxxpHDIs_lvg",
    "outputId": "412c65c1-7c24-4b9a-f5cf-8f98dd648fe4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_theme(font_scale=1.5)\n",
    "mpl.rcParams[\"font.family\"] = [\"sans-serif\"]\n",
    "mpl.rcParams[\"font.sans-serif\"] = [\n",
    "    \"Fira Sans\",\n",
    "    \"Computer Modern Sans Serif\",\n",
    "    \"DejaVu Sans\",\n",
    "    \"Verdana\",\n",
    "    \"Arial\",\n",
    "    \"Helvetica\",\n",
    "]\n",
    "# plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots path: `/home/kir0ul/Projects/RL_Olfaction/TriangleTask/plots`\n"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = Path(\"env\").parent\n",
    "PLOTS_PATH = ROOT_PATH / \"plots\"\n",
    "print(f\"Plots path: `{PLOTS_PATH.absolute()}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_plots():\n",
    "    if not PLOTS_PATH.exists():\n",
    "        os.mkdir(PLOTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NP28hCqLiVSy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params(seed=42, rng=None, n_runs=1, total_episodes=1000, epsilon=0.2, gamma=0.9, alpha=0.1, nLayers=5, nHiddenUnits=27, n_observations=None, n_actions=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Params(\n",
    "    seed=42,\n",
    "    n_runs=1,\n",
    "    total_episodes=200,\n",
    "    epsilon=0.2,\n",
    "    alpha=0.1,\n",
    "    gamma=0.9,\n",
    "    nHiddenUnits=5 * 5 + 2,\n",
    ")\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NP28hCqLiVSy"
   },
   "outputs": [],
   "source": [
    "# # Set the seed\n",
    "# p.rng = np.random.default_rng(p.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fz-X3HTQueX"
   },
   "source": [
    "## The environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "env = WrappedEnvironment(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 4\n",
      "Number of observations: 2\n"
     ]
    }
   ],
   "source": [
    "# Get number of actions\n",
    "# n_actions = env.action_space.n\n",
    "p.n_actions = env.numActions\n",
    "\n",
    "# Get the number of state observations\n",
    "# state, info = env.reset()\n",
    "state = env.reset()\n",
    "p.n_observations = len(state)\n",
    "\n",
    "print(f\"Number of actions: {p.n_actions}\")\n",
    "print(f\"Number of observations: {p.n_observations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, n_units=16):\n",
    "        super(DQN, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_observations, n_units),\n",
    "            nn.Linear(n_units, n_units),\n",
    "            nn.Linear(n_units, n_units),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(n_units, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=27, bias=True)\n",
       "    (1): Linear(in_features=27, out_features=27, bias=True)\n",
       "    (2): Linear(in_features=27, out_features=27, bias=True)\n",
       "    (3): Linear(in_features=27, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DQN(\n",
    "    n_observations=p.n_observations, n_actions=p.n_actions, n_units=p.nHiddenUnits\n",
    ").to(device)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Parameters sizes summary:\n",
      "[torch.Size([27, 2]), torch.Size([27]), torch.Size([27, 27]), torch.Size([27]), torch.Size([27, 27]), torch.Size([27]), torch.Size([4, 27]), torch.Size([4])]\n"
     ]
    }
   ],
   "source": [
    "# print(\"Model parameters:\")\n",
    "# print(list(net.parameters()))\n",
    "print(\"\\n\\nParameters sizes summary:\")\n",
    "print([item.shape for item in net.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DQN                                      [4]                       --\n",
       "├─Sequential: 1-1                        [4]                       --\n",
       "│    └─Linear: 2-1                       [27]                      81\n",
       "│    └─Linear: 2-2                       [27]                      756\n",
       "│    └─Linear: 2-3                       [27]                      756\n",
       "│    └─Linear: 2-4                       [4]                       112\n",
       "==========================================================================================\n",
       "Total params: 1,705\n",
       "Trainable params: 1,705\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.04\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(net, input_size=[state.shape], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: True\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.1\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.AdamW(net.parameters(), lr=p.alpha, amsgrad=True)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyEdi0gGiV1z",
    "outputId": "b644224d-13ff-4806-d751-0088efc24f85"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    def __init__(\n",
    "        self,\n",
    "        epsilon,\n",
    "        rng=None,\n",
    "    ):\n",
    "        self.epsilon = epsilon\n",
    "        # if rng:\n",
    "        #     self.rng = rng\n",
    "\n",
    "    def choose_action(self, action_space, state, state_action_values):\n",
    "        \"\"\"Choose an action a in the current world state (s)\"\"\"\n",
    "\n",
    "        def sample(action_space):\n",
    "            return random_choice(action_space)\n",
    "\n",
    "        # # First we randomize a number\n",
    "        # if hasattr(self, \"rng\"):\n",
    "        #     explor_exploit_tradeoff = self.rng.uniform(0, 1)\n",
    "        # else:\n",
    "        #     explor_exploit_tradeoff = np.random.uniform(0, 1)\n",
    "        explor_exploit_tradeoff = torch.rand(1)\n",
    "\n",
    "        # Exploration\n",
    "        if explor_exploit_tradeoff.item() < self.epsilon:\n",
    "            # action = action_space.sample()\n",
    "            action = sample(action_space)\n",
    "\n",
    "        # Exploitation (taking the biggest Q-value for this state)\n",
    "        else:\n",
    "            # Break ties randomly\n",
    "            # If all actions are the same for this state we choose a random one\n",
    "            # (otherwise `argmax()` would always take the first one)\n",
    "            if torch.all(state_action_values == state_action_values[0]):\n",
    "                action = sample(action_space)\n",
    "            else:\n",
    "                action = torch.argmax(state_action_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyEdi0gGiV1z",
    "outputId": "b644224d-13ff-4806-d751-0088efc24f85"
   },
   "outputs": [],
   "source": [
    "explorer = EpsilonGreedy(epsilon=p.epsilon, rng=p.rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyEdi0gGiV1z",
    "outputId": "b644224d-13ff-4806-d751-0088efc24f85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 1/1 - Episodes:  50%|███████████████████████████▊                            | 496/1000 [02:12<2:20:20, 16.71s/it]"
     ]
    }
   ],
   "source": [
    "rewards = torch.zeros((p.total_episodes, p.n_runs), device=device)\n",
    "steps = torch.zeros((p.total_episodes, p.n_runs), device=device)\n",
    "episodes = torch.arange(p.total_episodes, device=device)\n",
    "all_states = []\n",
    "all_actions = []\n",
    "losses = [[] for _ in range(p.n_runs)]\n",
    "episode_durations = []\n",
    "\n",
    "for run in range(p.n_runs):  # Run several times to account for stochasticity\n",
    "    # Reset model\n",
    "    net = DQN(\n",
    "        n_observations=p.n_observations, n_actions=p.n_actions, n_units=p.nHiddenUnits\n",
    "    ).to(device)\n",
    "    optimizer = optim.AdamW(net.parameters(), lr=p.alpha, amsgrad=True)\n",
    "\n",
    "    for episode in tqdm(\n",
    "        episodes, desc=f\"Run {run+1}/{p.n_runs} - Episodes\", leave=False\n",
    "    ):\n",
    "        state = env.reset()  # Reset the environment\n",
    "        state = state.clone().float().detach().to(device)\n",
    "        step_count = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        while not done:\n",
    "            state_action_values = net(state).to(device)\n",
    "            action = explorer.choose_action(\n",
    "                action_space=env.action_space,\n",
    "                state=state,\n",
    "                state_action_values=state_action_values,\n",
    "            )\n",
    "\n",
    "            # # Record states and actions\n",
    "            all_states.append(state)\n",
    "            all_actions.append(Actions(action.item()).name)\n",
    "\n",
    "            observation, reward, done = env.step(\n",
    "                action=action.item(), current_state=state\n",
    "            )\n",
    "            # if reward != 0:\n",
    "            #     ipdb.set_trace()\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            # if terminated:\n",
    "            if done:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = observation.clone().float().detach()\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # # Compute V(s_{t+1}) for all next states.\n",
    "            # # Expected values of actions are computed based\n",
    "            # # on the \"older\" network, then selecting their best reward.\n",
    "            # # Should be either the expected state value or 0 in case the state is final.\n",
    "            # next_state_values = torch.zeros_like(state_action_values, device=device)\n",
    "            # with torch.no_grad():\n",
    "            #     next_state_values = net(state).max(1)[0]\n",
    "\n",
    "            # Update only the weights related to action taken\n",
    "            next_state_values = state_action_values\n",
    "            if done:\n",
    "                # next_state_values[action.item()] = 0\n",
    "                next_state_values = torch.zeros(1, device=device)\n",
    "            else:\n",
    "                # next_state_values[action.item()] = net(state).max()\n",
    "                next_state_values = net(state).max()\n",
    "\n",
    "            # Compute the expected Q values\n",
    "            expected_state_action_values = reward + p.gamma * next_state_values\n",
    "\n",
    "            # Compute loss\n",
    "            criterion = nn.MSELoss()\n",
    "            # loss = criterion(state_action_values, expected_state_action_values)\n",
    "            # if state_action_values[action.item()].shape != expected_state_action_values.shape:\n",
    "            loss = criterion(\n",
    "                state_action_values[action].unsqueeze(-1),\n",
    "                expected_state_action_values,\n",
    "            )\n",
    "\n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # In-place gradient clipping\n",
    "            torch.nn.utils.clip_grad_value_(net.parameters(), 100)\n",
    "            optimizer.step()\n",
    "\n",
    "            if done:\n",
    "                episode_durations.append(step_count + 1)\n",
    "                # plot_durations()\n",
    "\n",
    "            total_rewards += reward\n",
    "            step_count += 1\n",
    "            losses[run].append(loss.item())\n",
    "\n",
    "        rewards[episode, run] = total_rewards\n",
    "        steps[episode, run] = step_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grads_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grads_metrics[\"avg_rolling\"] = np.nan\n",
    "# grads_metrics[\"std_rolling\"] = np.nan\n",
    "# for id in grads_metrics.id.unique():\n",
    "#     grads_metrics.loc[grads_metrics[grads_metrics[\"id\"] == id].index, \"avg_rolling\"] = (\n",
    "#         grads_metrics.loc[grads_metrics[grads_metrics[\"id\"] == id].index, \"avg\"]\n",
    "#         .rolling(20)\n",
    "#         .mean()\n",
    "#     )\n",
    "#     grads_metrics.loc[grads_metrics[grads_metrics[\"id\"] == id].index, \"std_rolling\"] = (\n",
    "#         grads_metrics.loc[grads_metrics[grads_metrics[\"id\"] == id].index, \"std\"]\n",
    "#         .rolling(20)\n",
    "#         .mean()\n",
    "#     )\n",
    "# grads_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyEdi0gGiV1z",
    "outputId": "b644224d-13ff-4806-d751-0088efc24f85",
    "tags": []
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_weights(weights_metrics):\n",
    "#     \"\"\"Plot the weights.\"\"\"\n",
    "#     fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "#     sns.lineplot(x=\"steps_global\", y=\"avg\", hue=\"id\", data=weights_metrics, ax=ax[0])\n",
    "#     ax[0].set(ylabel=\"Weights (avg)\")\n",
    "#     ax[0].set(xlabel=\"Steps\")\n",
    "\n",
    "#     sns.lineplot(x=\"steps_global\", y=\"std\", hue=\"id\", data=weights_metrics, ax=ax[1])\n",
    "#     ax[1].set(ylabel=\"Weights (std)\")\n",
    "#     ax[1].set(xlabel=\"Steps\")\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_weights(weights_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_gradients(grads_metrics):\n",
    "#     \"\"\"Plot the gradienta.\"\"\"\n",
    "#     fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "#     sns.lineplot(\n",
    "#         x=\"steps_global\",\n",
    "#         y=\"avg_rolling\",\n",
    "#         hue=\"id\",\n",
    "#         data=grads_metrics,\n",
    "#         ax=ax[0],\n",
    "#         palette=sns.color_palette()[0 : len(grads_metrics.id.unique())],\n",
    "#     )\n",
    "#     ax[0].set(ylabel=\"Gradients (avg)\")\n",
    "#     ax[0].set(xlabel=\"Steps\")\n",
    "\n",
    "#     sns.lineplot(\n",
    "#         x=\"steps_global\",\n",
    "#         y=\"std_rolling\",\n",
    "#         hue=\"id\",\n",
    "#         data=grads_metrics,\n",
    "#         ax=ax[1],\n",
    "#         palette=sns.color_palette()[0 : len(grads_metrics.id.unique())],\n",
    "#     )\n",
    "#     ax[1].set(ylabel=\"Gradients (std)\")\n",
    "#     ax[1].set(xlabel=\"Steps\")\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_gradients(grads_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(episodes, p, rewards, steps):\n",
    "    \"\"\"Convert the results of the simulation in dataframes.\"\"\"\n",
    "    res = pd.DataFrame(\n",
    "        data={\n",
    "            \"Episodes\": episodes.tile(p.n_runs),\n",
    "            \"Rewards\": rewards.T.flatten(),\n",
    "            \"Steps\": steps.T.flatten(),\n",
    "        }\n",
    "    )\n",
    "    # res[\"cum_rewards\"] = rewards.cumsum(axis=0).flatten(order=\"F\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = postprocess(episodes, p, rewards, steps)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we will plot the distributions of states and actions\n",
    "with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_states_actions_distribution(states, actions):\n",
    "    \"\"\"Plot the distributions of states and actions.\"\"\"\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(13, 5))\n",
    "    # sns.histplot(data=states, ax=ax[0])\n",
    "    ax[0].set_title(\"States\")\n",
    "    sns.histplot(data=actions, ax=ax[1])\n",
    "    ax[1].set_xticks(\n",
    "        [item.value for item in Actions], labels=[item.name for item in Actions]\n",
    "    )\n",
    "    ax[1].set_title(\"Actions\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_states_actions_distribution(all_states, all_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "pk8lHOjiL7fa",
    "outputId": "0474335e-bf29-4be1-d877-e6adc5a13afc"
   },
   "outputs": [],
   "source": [
    "def plot_steps_and_rewards(df):\n",
    "    \"\"\"Plot the steps and rewards from dataframes.\"\"\"\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    sns.lineplot(data=df, x=\"Episodes\", y=\"Rewards\", ax=ax[0])\n",
    "    ax[0].set(\n",
    "        ylabel=f\"Rewards\\naveraged over {p.n_runs} runs\"\n",
    "        if p.n_runs > 1\n",
    "        else \"Steps number\"\n",
    "    )\n",
    "\n",
    "    sns.lineplot(data=df, x=\"Episodes\", y=\"Steps\", ax=ax[1])\n",
    "    ax[1].set(\n",
    "        ylabel=f\"Steps number\\naveraged over {p.n_runs} runs\"\n",
    "        if p.n_runs > 1\n",
    "        else \"Steps number\"\n",
    "    )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xj1z5ob10ltw"
   },
   "outputs": [],
   "source": [
    "plot_steps_and_rewards(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "for idx, loss in enumerate(losses):\n",
    "    current_loss = torch.tensor(loss, device=device)\n",
    "    losses_rolling_avg = nn.functional.avg_pool1d(\n",
    "        current_loss.view(1, 1, -1), kernel_size=window_size\n",
    "    ).squeeze()\n",
    "    tmp_df = pd.DataFrame(\n",
    "        data={\n",
    "            \"Run\": idx * torch.ones(len(losses_rolling_avg), device=device).int(),\n",
    "            \"Steps\": torch.arange(0, len(losses_rolling_avg), device=device),\n",
    "            \"Loss\": losses_rolling_avg,\n",
    "        }\n",
    "    )\n",
    "    if idx == 0:\n",
    "        loss_df = tmp_df\n",
    "    else:\n",
    "        loss_df = pd.concat((loss_df, tmp_df))\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(data=loss_df, x=\"Steps\", y=\"Loss\", ax=ax)\n",
    "ax.set(\n",
    "    ylabel=f\"$Log_{{10}}(\\\\text{{Loss}})$\\naveraged over {p.n_runs} runs\"\n",
    "    if p.n_runs > 1\n",
    "    else \"$Log_{10}(\\\\text{Loss})$\"\n",
    ")\n",
    "ax.set(xlabel=\"Steps\")\n",
    "ax.set(yscale=\"log\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
