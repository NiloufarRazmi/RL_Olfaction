{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d1a9876-eccf-4a9f-9ab7-871420850917",
   "metadata": {},
   "source": [
    "# Ablation experiment\n",
    "\n",
    "**Goal of the experiment:** silence or randomize one set of coordinates (Cartesian/polar) to see the effects\n",
    "\n",
    "Potential metrics:\n",
    "- performance histogram\n",
    "- % correct\n",
    "- shift in behavior\n",
    "- Steps number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c36036ed-d545-4741-b3d3-0ad5fe724f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import make_deterministic, random_choice\n",
    "from agent import EpsilonGreedy, neural_network\n",
    "import utils\n",
    "from environment import CONTEXTS_LABELS, Actions, Cues, DuplicatedCoordsEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94766291-0065-4ce4-9860-eb85065f2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b1e9cba-9b8f-459d-a4f5-bc68b2cb1c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = Path(\"..\") / \"save\"\n",
    "save_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46ac9dbd-d9e5-4622-a470-20e1a5d30766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = save_path / \"2025-03-08_01-44-12_EastWest_save-all-agents\"\n",
    "# data_dir = save_path / \"2025-03-08_01-47-50_LeftRight_save-all-agents\"\n",
    "data_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "365b8e4a-f152-4b88-a196-4c2b62f7c388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = data_dir / \"data.tar\"\n",
    "data_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b77c9e7f-e51f-49d0-b4bf-5cbd09f65928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = data_dir / \"trained-agent-state-0.pt\"\n",
    "model_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4886e019-754b-4059-b749-f49cc9840595",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = torch.load(data_path, weights_only=False, map_location=DEVICE)\n",
    "\n",
    "# Access individual arrays by their names\n",
    "p = data_dict[\"p\"]\n",
    "env = data_dict[\"env\"]\n",
    "net = data_dict[\"net\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad40683-a750-4804-89f9-4e3db5b4c5a8",
   "metadata": {},
   "source": [
    "## Inference loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "714abe54-7b7e-4b06-b445-3e95f0abaccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=512, bias=True)\n",
       "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (6): ReLU()\n",
       "    (7): Linear(in_features=512, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(model_path, weights_only=True, map_location=DEVICE))\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cfcf5dd4-336c-4d35-a881-b3ae94dfb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablate_state(state, keep, silence=True):\n",
    "    new_state = state\n",
    "    if keep == \"cartesian\":\n",
    "        idx = np.arange(9, 19)\n",
    "    elif keep == \"polar\":\n",
    "        idx = np.arange(1, 9)\n",
    "    else:\n",
    "        raise ValueError(\"The state to keep can only be either 'polar' or 'cartesian'\")\n",
    "\n",
    "    if silence:\n",
    "        new_state[idx] = 0\n",
    "    else:\n",
    "        new_state[idx] = torch.rand(len(idx), device=DEVICE)\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "50186a54-0dd4-4285-a7d3-8b0ea040fab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  2.0000,  2.0000, -0.0000, -1.0000,  2.0000,  2.0000,  0.0000,\n",
       "         1.0000,  2.8284,  0.7071,  0.7071, -0.7071, -0.7071,  2.8284,  0.7071,\n",
       "         0.7071,  0.7071,  0.7071])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()  # Reset the environment\n",
    "state = state.clone().float().detach().to(DEVICE)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6accd8c4-d1ed-4774-ba3e-08eada9bca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = \"cartesian\"\n",
    "# keep = \"polar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7c11055b-c03b-422f-b406-b7cfdf482014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  2.,  2., -0., -1.,  2.,  2.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_ablated = ablate_state(state=state, keep=keep, silence=True)\n",
    "state_ablated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8cd90c-975b-4d03-b580-57199f92bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    \"Transition\", (\"state\", \"action\", \"reward\", \"next_state\", \"done\")\n",
    ")\n",
    "\n",
    "for episode in tqdm(\n",
    "    episodes, desc=f\"Run {run + 1}/{p.n_runs} - Episodes\", leave=False\n",
    "    ):\n",
    "    state = env.reset()  # Reset the environment\n",
    "    state = state.clone().float().detach().to(DEVICE)\n",
    "    step_count = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    loss = torch.ones(1, device=DEVICE) * torch.nan\n",
    "    \n",
    "    while not done:\n",
    "        state_action_values = net(state).to(DEVICE)  # Q(s_t)\n",
    "        action = explorer.choose_action(\n",
    "            action_space=env.action_space,\n",
    "            state=state,\n",
    "            state_action_values=state_action_values,\n",
    "        ).item()\n",
    "    \n",
    "        # Record states and actions\n",
    "        all_states[run][episode].append(state.cpu())\n",
    "        all_actions[run][episode].append(Actions(action).name)\n",
    "    \n",
    "        next_state, reward, done = env.step(action=action, current_state=state)\n",
    "    \n",
    "        # Store transition in replay buffer\n",
    "        # [current_state (2 or 28 x1), action (1x1), next_state (2 or 28 x1),\n",
    "        # reward (1x1), done (1x1 bool)]\n",
    "        done = torch.tensor(done, device=DEVICE).unsqueeze(-1)\n",
    "        replay_buffer.append(\n",
    "            Transition(\n",
    "                state,\n",
    "                action,\n",
    "                reward,\n",
    "                next_state,\n",
    "                done,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "        # Start training when `replay_buffer` is full\n",
    "        if len(replay_buffer) == p.replay_buffer_max_size:\n",
    "            transitions = utils.random_choice(\n",
    "                replay_buffer,\n",
    "                length=len(replay_buffer),\n",
    "                num_samples=p.batch_size,\n",
    "                generator=generator,\n",
    "            )\n",
    "            batch = Transition(*zip(*transitions, strict=True))\n",
    "            state_batch = torch.stack(batch.state)\n",
    "            action_batch = torch.tensor(batch.action, device=DEVICE)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "            # next_state_batch = torch.stack(batch.next_state)\n",
    "            # done_batch = torch.cat(batch.done)\n",
    "    \n",
    "            # See DQN paper for equations: https://doi.org/10.1038/nature14236\n",
    "            state_action_values_sampled = net(state_batch).to(DEVICE)  # Q(s_t)\n",
    "            state_action_values = torch.gather(\n",
    "                input=state_action_values_sampled,\n",
    "                dim=1,\n",
    "                index=action_batch.unsqueeze(-1),\n",
    "            ).squeeze()  # Q(s_t, a)\n",
    "    \n",
    "            # Compute a mask of non-final states and concatenate\n",
    "            # the batch elements\n",
    "            # (a final state would've been the one after which simulation ended)\n",
    "            non_final_mask = torch.tensor(\n",
    "                tuple(map(lambda s: not s, batch.done)),\n",
    "                device=DEVICE,\n",
    "                dtype=torch.bool,\n",
    "            )\n",
    "            non_final_next_states = torch.stack(\n",
    "                [s[1] for s in zip(batch.done, batch.next_state) if not s[0]]\n",
    "            )\n",
    "    \n",
    "            # Compute V(s_{t+1}) for all next states.\n",
    "            # Expected values of actions for non_final_next_states are computed\n",
    "            # based on the \"older\" target_net;\n",
    "            # selecting their best reward with max(1).values\n",
    "            # This is merged based on the mask,\n",
    "            # such that we'll have either the expected\n",
    "            # state value or 0 in case the state was final.\n",
    "            next_state_values = torch.zeros(p.batch_size, device=DEVICE)\n",
    "            if non_final_next_states.numel() > 0 and non_final_mask.numel() > 0:\n",
    "                with torch.no_grad():\n",
    "                    next_state_values[non_final_mask] = (\n",
    "                        target_net(non_final_next_states).max(1).values\n",
    "                    )\n",
    "            # Compute the expected Q values\n",
    "            expected_state_action_values = reward_batch + (\n",
    "                next_state_values * p.gamma\n",
    "            )\n",
    "    \n",
    "            # Compute loss\n",
    "            # criterion = nn.MSELoss()\n",
    "            criterion = nn.SmoothL1Loss()\n",
    "            loss = criterion(\n",
    "                input=state_action_values,  # prediction\n",
    "                target=expected_state_action_values,  # target/\"truth\" value\n",
    "            )  # TD update\n",
    "    \n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(\n",
    "                net.parameters(), 100\n",
    "            )  # In-place gradient clipping\n",
    "            optimizer.step()\n",
    "    \n",
    "            # # Reset the target network\n",
    "            # if step_count % p.target_net_update == 0:\n",
    "            #     target_net.load_state_dict(net.state_dict())\n",
    "    \n",
    "            # Soft update of the target network's weights\n",
    "            # θ′ ← τ θ + (1 −τ )θ′\n",
    "            target_net_state_dict = target_net.state_dict()\n",
    "            net_state_dict = net.state_dict()\n",
    "            for key in net_state_dict:\n",
    "                target_net_state_dict[key] = net_state_dict[\n",
    "                    key\n",
    "                ] * p.tau + target_net_state_dict[key] * (1 - p.tau)\n",
    "            target_net.load_state_dict(target_net_state_dict)\n",
    "    \n",
    "            losses[run].append(loss.item())\n",
    "    \n",
    "            weights, biases = utils.collect_weights_biases(net=net)\n",
    "            weights_val_stats = utils.params_df_stats(\n",
    "                weights, key=\"val\", current_df=weights_grad_stats\n",
    "            )\n",
    "            biases_val_stats = utils.params_df_stats(\n",
    "                biases, key=\"val\", current_df=biases_val_stats\n",
    "            )\n",
    "            biases_grad_stats = utils.params_df_stats(\n",
    "                biases, key=\"grad\", current_df=biases_grad_stats\n",
    "            )\n",
    "            weights_grad_stats = utils.params_df_stats(\n",
    "                weights, key=\"grad\", current_df=weights_val_stats\n",
    "            )\n",
    "    \n",
    "        total_rewards += reward\n",
    "        step_count += 1\n",
    "    \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "    \n",
    "        explorer.epsilon = explorer.update_epsilon(episode)\n",
    "        epsilons.append(explorer.epsilon)\n",
    "    \n",
    "    all_states[run][episode].append(state.cpu())\n",
    "    rewards[episode, run] = total_rewards\n",
    "    steps[episode, run] = step_count\n",
    "    logger.info(\n",
    "        f\"Run: {run + 1}/{p.n_runs} - Episode: {episode + 1}/{p.total_episodes}\"\n",
    "        f\" - Steps: {step_count} - Loss: {loss.item()}\"\n",
    "        f\" - epsilon: {explorer.epsilon}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
