#+TITLE: Implementation discussion
#+SUBTITLE: Cartesian/polar duplicated coordinates experiment
#+author: Andrea Pierr√©
#+date: August 30, 2024
* HEADER :noexport:
#+SETUPFILE: ./style.org

* Status
** Status
- Environment: done
  - Main class with origin (0,0) at the center of the environment
  - Subclass that converts the coordinates of the agent to North & South ports in Cartesian and polar coordinates
  - 44 unit tests to check that the code does what it's supposed to do and that the agent is where it's supposed to be
- Visualization: WIP
- Training: to-do
* Discussion
** Should there be a backward action?
#+ATTR_LaTeX: :height 0.8\textheight
[[file:img/RL_env-cartesian-polar.drawio.pdf]]
** What should be part of the state?
:PROPERTIES:
:BEAMER_opt: fragile
:END:
#+begin_export latex
\scriptsize
\begin{lstlisting}[language={Python}]
state = TensorDict(
    {
        "cue": torch.tensor(Cues.NoOdor.value, device=DEVICE).unsqueeze(-1),
        "x": torch.tensor([x], device=DEVICE),
        "y": torch.tensor([y], device=DEVICE),
        "direction": torch.tensor([direction], device=DEVICE),
    },
    batch_size=[1],
    device=DEVICE,
)
\end{lstlisting}
\normalsize
#+end_export
\to Should head direction be part of the state?
** Tiles on the diagonal
*** Left
:PROPERTIES:
:BEAMER_col: 0.6
:BEAMER_opt: [t]
:END:
[[file:img/policy.png]]
*** Right
:PROPERTIES:
:BEAMER_col: 0.4
:BEAMER_opt: [t]
:END:
[[file:img/task-left-right.jpeg]]
** Length rounding in polar coordinates?
- For step 1 tiles
- For step 0.5 tiles
[[file:img/polar-discretized-length.drawio.pdf]]
** Which architecture for the network?
- 2 tasks (East/West & Left/Right)
- upper/lower triangle
- 4 head directions
- 5 discretized x coordinates (Cartesian)
- 5 discretized y coordinates (Cartesian)
- 360 discretized angles (polar)
- 50 discretized lengths (polar)

** \nbsp{}
:PROPERTIES:
:BEAMER_opt: standout
:END:
Thanks!

* COMMENT Add plain option to Beamer TOC
% Local variables:
% org-beamer-outline-frame-options: "plain"
% End:

* Feedback :noexport:
** Arena state space
** Action space
** IDEA How this task structure might employ different representation/transformation of the action space
** IDEA Hypothesis:
- The network will use the most efficient info
- Does changing the reward mapping/task make a change in the weights indicating the agent use only certain info?
** TODO Direction replaced could be replaced by angle from the port
** TODO Ports should be ego encoded
** TODO Angle relative from the port
Difference from the agent to the port
** 2 units for head direction
*** cos
*** sin
** 2 hidden layers
** TODO head direction relative to the agent
*** Need to keep a fixed internal direction?
*** But feed a relative head direction to the network?
** TODO Remove rounding, pass true floating value to the network?
