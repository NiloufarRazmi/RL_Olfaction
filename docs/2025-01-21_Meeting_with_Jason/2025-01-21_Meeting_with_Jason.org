#+TITLE: DRL project update
#+SUBTITLE: Cartesian/polar duplicated coordinates experiment
#+author: Andrea Pierr√©
#+date: January 21, 2025
* HEADER :noexport:
#+SETUPFILE: ./style.org

* Current status
** Current status
#+ATTR_LaTeX: :height 0.6\textheight
[[file:img/RL_env-cartesian-polar.drawio.png]]
- Environment rewrite: done
- Training: ~4.5 hours to train 30 agents on both tasks on Oscar
- Analysis: WIP
** State space & network architecture
#+ATTR_LaTeX: :height 0.95\textheight
[[file:img/state-space-nn.png]]
** Training
East/West
#+ATTR_LaTeX: :height 0.35\textheight
[[file:img/steps-and-rewards-EastWest.png]]
Left/Right
#+ATTR_LaTeX: :height 0.35\textheight
[[file:img/steps-and-rewards-LeftRight.png]]
** Training checks - East/West
#+ATTR_LaTeX: :width \textwidth
[[file:img/steps-and-rewards-distrib-EastWest.png]]
*** Left
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_opt: [c]
:END:
#+ATTR_LaTeX: :width 0.8\textwidth
[[file:img/loss-EastWest.png]]
*** Right
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_opt: [c]
:END:
#+ATTR_LaTeX: :width 0.65\textwidth
[[file:img/actions-distribution-EastWest.png]]
** Training checks - Left/Right
#+ATTR_LaTeX: :width \textwidth
[[file:img/steps-and-rewards-distrib-LeftRight.png]]
*** Left
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_opt: [c]
:END:
#+ATTR_LaTeX: :width 0.8\textwidth
[[file:img/loss-LeftRight.png]]
*** Right
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_opt: [c]
:END:
#+ATTR_LaTeX: :width 0.65\textwidth
[[file:img/actions-distribution-LeftRight.png]]
** Agent behavior
*** Left
:PROPERTIES:
:BEAMER_col: 0.3
:BEAMER_opt: [c]
:END:
#+ATTR_LaTeX: :height 0.28\textheight
[[file:img/trajectory-0-0-LeftRight.png]]
#+ATTR_LaTeX: :height 0.28\textheight
[[file:img/trajectory-0-120-LeftRight.png]]
#+ATTR_LaTeX: :height 0.28\textheight
[[file:img/trajectory-0-350-LeftRight.png]]
*** Center
:PROPERTIES:
:BEAMER_col: 0.3
:BEAMER_opt: [c]
:END:
#+ATTR_LaTeX: :height 0.28\textheight
[[file:img/trajectory-0-0-EastWest.png]]
#+ATTR_LaTeX: :height 0.28\textheight
[[file:img/trajectory-0-120-EastWest.png]]
#+ATTR_LaTeX: :height 0.28\textheight
[[file:img/trajectory-0-350-EastWest.png]]
*** Right
:PROPERTIES:
:BEAMER_col: 0.25
:BEAMER_opt: [c]
:END:
#+ATTR_LaTeX: :width \linewidth
[[file:img/states-occupancy-EastWest.png]]
#+ATTR_LaTeX: :width \linewidth
[[file:img/states-occupancy-LefRight.png]]
* How to get insights at what the network learn?
** Activations learned - East/West
#+ATTR_LaTeX: :height 0.9\textheight
[[file:img/activations-learned-EastWest.png]]
** Activations learned - Left/Right
#+ATTR_LaTeX: :height 0.9\textheight
[[file:img/activations-learned-LeftRight.png]]
** Use the behavior as proxy
:PROPERTIES:
:BEAMER_act: [<+->]
:END:
- Silence the Cartesian/polar part of the input on a trained agent and look at how the agent behaves (x4 experiments)
- Expectation:
  - Left/right task:
    - With the *Cartesian* inputs silenced \to the agent can solve the task
    - With the *polar* inputs silenced \to the agent struggle to solve the task
  - East/west task:
    - With the *polar* inputs silenced \to the agent can solve the task
    - With the *Cartesian* inputs silenced \to the agent struggle to solve the task
- Any other approach we could use?
** Neural representations?
[[file:~/Projects/RL_Olfaction/docs/expected cart-polar activations on both tasks.png]]
- Need for some causal framework?
* COMMENT Add plain option to Beamer TOC
% Local variables:
% org-beamer-outline-frame-options: "plain"
% End:

* Feedback :noexport:
** IDEA How this task structure might employ different representation/transformation of the action space
** IDEA Hypothesis:
- The network will use the most efficient info
- Does changing the reward mapping/task make a change in the weights indicating the agent use only certain info?

** Need for some causal framework?
- [[https://doi.org/10.1038/s42256-020-0218-x]]
- [[https://doi.org/10.31235/osf.io/aeszf]]
- [[https://github.com/kochbj/Deep-Learning-for-Causal-Inference]]

** Ablation study
*** Performance histogram
*** Percent correct
*** Shift in behavior
*** Number of steps
** Conflicting information
*** 1 set of coord is right, the other is wrong
*** See if the agent follow one or the other
** Cosine input or x node
*** What's the distrib coming on the sine
*** Multidim problem
*** Look for sensitivity coming in the first layer
** Derive of any input wrt input
*** Avg over playback of the agent
*** How sensitive is the agent for this task
** XAI
*** Deepdream
**** What input maximally drive a particular node
** Other type of clustering?
*** Not hierarchical clustering
*** Fix some condition
** Try to understand the weights more than the activations
** Weights are the structure of the network
** Target; E/W or L/R
*** Supervised clustering
*** Across all the layers
** Cluster by action space
** Cluster if moved the agent closer to goal or not
*** Break by task phases
*** before cue after cue
